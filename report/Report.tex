\documentclass[11pt]{report}
\usepackage{mcode}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage[final]{pdfpages}
\setcounter{secnumdepth}{-1}
\usepackage{amsfonts}
\usepackage{float}
\pagestyle{fancy}
\usepackage{url}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{alltt}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{xr}
\usepackage{amssymb}
\usepackage{courier}
\nonstopmode
\pagestyle{fancy}
\lhead{Nikolaj Friis Ã˜stergaard - ltm741}
\rhead{\today}
\newcommand{\then}{\rightarrow}
\DeclareMathOperator*{\Bigcdot}{\scalerel*{\cdot}{\bigodot}}
\begin{document}

\section{2 Image filtering and enhancement}

\subsection{2.1}
The approximation for the x value uses the difference between the pixel before and the pixel after. The y value is calculated the same way. This result in the kernel:
$$
K =
\begin{bmatrix}
  -1 & 0 & 1
 \end{bmatrix}
$$

The imfilter function can either use correlation or convolution, the default is correlation. Correlation and convolution are almost the same, but convolution flips the kernel, on both axis. So the resulting kernel for the simle approximation approach is with convolution is
$$
K =
\begin{bmatrix}
  1 & 0 & -1
 \end{bmatrix}
$$
For kernels like gaussian, which are symetrical around the center, it does not matter which we choose to use, since kernels that will run over the image for the correlation and convolution will be the same.
\subsection{2.2}

The kernel in 4.5.2.1 shows the kernel for the x derivative, and thus these will be used for discussion. Instead of just looking at the pixel to the right and the left, we also use the information about the row above and below. By using the row above and below, we don't interpret a single noise pixel as being part of an edge, since the pixels below and above does not indicate that there is a continued edge. This makes noise get lower values than real edge, which spanned multiple rows. The same logic is applied for the y derivative, where we look at the derivatives and both sides. Sobel weighs the current rows pixels higher the the ones below and above. Where Prewitt weigh them equally.

\subsection{2.3}
Figure \ref{fig:2_3_0} shows the eight.tif image with both salt and pepper noise and gaussian noise, filtered with mean and median filter. The first column shows the image with salt and pepper noise with a mean filter, the second column is the image with gaussian noise filtered with mean filter. The third column shows the iamge with salt and pepper noise, filtered with a median filter and the fourth column shows the image with gaussian noise, with a median filter. The first row has a windows size of 3, the second window size of 5 and the third a window size of 7. Looking at the first column we can see how increasing windows size improves the denoising, but also the edge blur a bit. In the second column we see again how increasing the windows size improves the denoising. For the median filter the salt and peper in column 3 is as good as gone in the first image, with windows size 3, increasing the window size only blurs the image a bit more, and makes the background brighter. In column 4 with gaussian noise we see that the increase in window size improves the denoising, while bluring the image, about as much as with the mean filter.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{fig23_0.jpg}
    \caption{mean and median filter on salt/peper noise and on gaussian noise for different window sizes}
    \label{fig:2_3_0}
\end{figure}

Figure \ref{fig:2_3} shows the computaional cost for increasing window sizes. Th blue line is median filter, red is mean filter. As we can see, median filter is generally more expensive to use, compared to the mean filter. Another thing to notice is that the median filter has a lower computational cost when the windows size is odd. It also looks like the median filter spikes more, and increases in computaional cost, when N becomes bigger, see when $N \geq 21$ on the graph, compared to the mean filter, which seems to be increasing at a steady pace.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{fig23.jpg}
    \caption{The computational cost of doing mean filter and median filter, based on window size.}
    \label{fig:2_3}
\end{figure}

\subsection{2.4}
Figure \ref{fig:2_4} shows the eight.tif, with salt and pepper noise, filtered with a gaussian kernel where $\sigma = 5$, with windows size in the range 3:19, with increments og 2, to keep the kernel diameter odd. We can see how the difference between image 5 and image 8, are less notacible then the difference between image 2 and image 5. This is because with a smaller windows, the image is blurred less. At some point the windows is so big, that the values furthest away from the center are so small, that their weight is neglectable, thus increasing the windows size show no notacible difference. And that is why its easier to see the difference between image 2 and 5 then image 5 and 8.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{fig24.jpg}
    \caption{9 Images with inceasing windows size starting at 3 ending at 19.}
    \label{fig:2_4}
\end{figure}



\subsection{2.5}

Figure \ref{fig:2_5} show the eight.tif, with salt and peper noise, filtered with a gaussian filter, with increasing $\sigma$ starting at 1 and ending at 17, in incremental steps of 2. The windows size N is 3*sigma + 1, and then rounding up to nearest odd integer. What we can see that increasing $\sigma$ and windows size, rapidly improves the noise reduction, but also blures the edges to an equal extend. In image 4, the noise is as good as gone, so from there on, the only difference is just that the edges in the following images are more blurred out. So increasing the $\sigma$ and windows size does makes the filter more effective at removing noise, but also makes finding edges harder. The noise removal is not something that improves the larger $\sigma$, since when sigma = 7 and window size = 23, the noise is gone, and we can't do better then remove it.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{fig25.jpg}
    \caption{9 Images with inceasing sigma and windows size starting at sigma = 1 and ending at sigma = 17, with increments of 2.}
    \label{fig:2_5}
\end{figure}

\section{3 Bonus question}

\subsection{3.1}

The bilateral filter is not linear, because of the non-linear dependency on the weights $w$. The term that differs from the usual Gaussian filter is the range intensity difference smoothing term $g_\tau$. The interpretation of the $\tau$ parameter is that it is the intensity smoothing parameter, which for low values weighs intensity differences higher and for high values weighs intensity differences the same. Bilateral filtering becomes usual Gaussian filtering in the limit of $\tau$ as it approaches $\infty$, because $g_\tau$ approaches 1.

\subsection{3.2}

\lstinputlisting{bilateral_filtering.m}\\
\\
The function first pads the image with zeroes to avoid out of bounds exceptions. It then computes the spatial gaussian filter. Finally it computes the filtered image.\\
\\
\subsection{3.3}

\begin{figure}[h]
\centering
\includegraphics{A3_3_1.png}
\includegraphics{A3_3_2.png}
\includegraphics{A3_3_3.png}
\includegraphics{A3_3_4.png}
\caption{Bilateral filtering and edge detection applied to \textit{eight.tif} corrupted by gaussian noise, for various $\sigma$ and $\tau$.}
\end{figure}

Bilateral filtering and edge detection applied to \textit{eight.tif} corrupted by gaussian noise, for various $\sigma$ and $\tau$ can be seen in figure 5. The first column is the filtered image, the second is \textit{Sobel} edge detection, the third is \textit{Prewitt} edge detection, and the fourth is \textit{Zero-cross} edge detection. The image in the first row and the first first column is the original unfiltered image. $\sigma = 1$ in rows 2, 5, and 8, $\sigma = 9$ in rows 3, 6, and 9, and $\sigma = 17$ in rows 4, 7, 10. $\tau = 10$ in rows 2, 3, and 4, $\tau = 50$ in rows 5, 6, and 7, and $\tau = 90$ in rows 8, 9, and 10.

The reduction in noise is increased as $\tau$ and $\sigma$ is increased, but so is the amount of blurring. The best settings amongst those applied seems to be $\sigma = 9$ and $\tau = 50$, which correspond to row 6.

\end{document}
